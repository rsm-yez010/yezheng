[
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYe Zheng\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYe Zheng\n\n\nApr 20, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YE ZHENG",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign by a liberal nonprofit organization in the United States. A total of 50,083 prior donors were randomly assigned to one of two main groups: a control group, which received a standard solicitation letter, and a treatment group, which received a matching grant offer.\nWithin the treatment group, participants were further randomized into subgroups with different matching ratios ($1:$1, $2:$1, or $3:$1), maximum matching amounts ($25,000, $50,000, $100,000, or unstated), and suggested donation levels (based on their previous giving history). Each donor received a four-page letter with content identical across groups, except for the inclusion or absence of matching grant language.\nThe goal of the experiment was to examine how changes in the price of giving (induced by the matching offers) affected the likelihood and amount of donations. The large sample size and real-world context made this a high-powered natural field experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#introduction",
    "href": "projects/project2/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign by a liberal nonprofit organization in the United States. A total of 50,083 prior donors were randomly assigned to one of two main groups: a control group, which received a standard solicitation letter, and a treatment group, which received a matching grant offer.\nWithin the treatment group, participants were further randomized into subgroups with different matching ratios ($1:$1, $2:$1, or $3:$1), maximum matching amounts ($25,000, $50,000, $100,000, or unstated), and suggested donation levels (based on their previous giving history). Each donor received a four-page letter with content identical across groups, except for the inclusion or absence of matching grant language.\nThe goal of the experiment was to examine how changes in the price of giving (induced by the matching offers) affected the likelihood and amount of donations. The large sample size and real-world context made this a high-powered natural field experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#data",
    "href": "projects/project2/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_stata(\"/Users/kris/Desktop/2025WORK/quarto_website/files/karlan_list_2007.dta\")\n\n\n\n\n\ndf.shape\n\n(50083, 51)\n\n\nThe dataset contains 50,083 observations and 51 variables, each representing a past donor who received a fundraising letter as part of a randomized field experiment. The data were collected to study how matching grants influence charitable giving behavior.\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\ndf['treatment'].value_counts() \n\ntreatment\n1    33396\n0    16687\nName: count, dtype: int64\n\n\nSummary statistics suggest most variables are binary or categorical indicators. Several variables such as cases, nonlit, and couple contain some missing values, which should be considered during modeling. The average suggested donation ranges widely, with variables like ask1 and askd1 showing values between 0 and 1 (indicating binary splits or quantiles).\nOverall, the data are rich in both experimental variation and individual-level covariates, suitable for analyzing treatment effects through regression and stratified comparisons.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo verify that the randomization was successful, I perform a balance test on several pre-treatment variables. Specifically, we compare the following variables across the treatment and control groups: - mrm2: months since last donation - freq: number of prior donations - hpa: highest previous contribution - female: gender indicator\nI conduct both t-tests and OLS regressions for each variable to assess statistical significance and confirm consistency across methods.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\nbalance_vars = ['mrm2', 'freq', 'hpa', 'female']\nttest_results = []\nreg_results = []\n\nfor var in balance_vars:\n    t0 = df[df['treatment'] == 0][var].dropna()\n    t1 = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = ttest_ind(t1, t0, equal_var=False)\n    ttest_results.append((var, t_stat, p_val))\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    reg_results.append((var, model.params['treatment'], model.pvalues['treatment']))\n\n\n\n\nAccording to the test result, none of the variables show statistically significant differences between treatment and control groups at the 5% level. The lowest p-value appears in the case of female (p ≈ 0.079), which is marginally significant under a 10% threshold, but still within an acceptable range for randomization.\nThese results confirm that the groups are generally well balanced across observed characteristics, supporting the internal validity of the experimental design. Importantly, our results align closely with those reported in Table 1 of Karlan and List (2007), which also finds no significant imbalance on these and other pre-treatment covariates.\n\nttest_df = pd.DataFrame(ttest_results, columns=[\"Variable\", \"T-statistic\", \"P-value\"])\nreg_df = pd.DataFrame(reg_results, columns=[\"Variable\", \"Coeff (Regression)\", \"P-value\"])\nttest_df.merge(reg_df, on=\"Variable\")\n\n\n\n\n\n\n\n\nVariable\nT-statistic\nP-value_x\nCoeff (Regression)\nP-value_y\n\n\n\n\n0\nmrm2\n0.119532\n0.904855\n0.013686\n0.904886\n\n\n1\nfreq\n-0.110845\n0.911740\n-0.011979\n0.911702\n\n\n2\nhpa\n0.970427\n0.331840\n0.637075\n0.345099\n\n\n3\nfemale\n-1.753513\n0.079523\n-0.007547\n0.078691"
  },
  {
    "objectID": "projects/project2/index.html#experimental-results",
    "href": "projects/project2/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nTo examine whether matching grants increase the likelihood of donating, I compare the donation response rate between the treatment and control groups. The plot below shows the proportion of individuals who made a donation in each group.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\n\nresponse_rate = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\nresponse_rate[\"Group\"] = response_rate[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nax = sns.barplot(data=response_rate, x=\"Group\", y=\"gave\", palette=\"Set2\")\n\nfor i, val in enumerate(response_rate[\"gave\"]):\n    ax.text(i, val + 0.0015, f\"{val:.2%}\", ha='center', va='bottom', fontsize=12)\n\nplt.title(\"Donation Response Rate by Group\", fontsize=16)\nplt.xlabel(\"Group\", fontsize=14)\nplt.ylabel(\"Proportion Who Donated\", fontsize=14)\nplt.ylim(0, response_rate[\"gave\"].max() * 1.3)\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe figure above compares the donation response rate between the treatment and control groups. The treatment group, which received a matching grant offer, had a donation rate of 2.20%, while the control group had a rate of 1.79%.\nAlthough the absolute difference in response rates appears modest, it represents a relative increase of over 22%, suggesting that the presence of a matching grant can meaningfully boost the likelihood of charitable giving. This provides early visual evidence supporting the hypothesis that price incentives (in the form of matching) increase donation behavior.\n\nWe now formally test whether individuals in the treatment group were significantly more likely to donate than those in the control group.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\nt1 = df[df[\"treatment\"] == 1][\"gave\"]\nt0 = df[df[\"treatment\"] == 0][\"gave\"]\nt_stat, p_val = ttest_ind(t1, t0, equal_var=False)\n\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoefs = model.params\npvals = model.pvalues\nconf_int = model.conf_int()\nresult_table = pd.DataFrame({\n    \"Coefficient\": coefs.round(4),\n    \"P-value\": pvals.round(4),\n    \"95% CI Lower\": conf_int[0].round(4),\n    \"95% CI Upper\": conf_int[1].round(4)\n})\n\n\n\n\n\nresult_table\n\n\n\n\n\n\n\n\nCoefficient\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.0179\n0.0000\n0.0157\n0.0200\n\n\ntreatment\n0.0042\n0.0019\n0.0015\n0.0068\n\n\n\n\n\n\n\nThe regression model estimates the effect of being in the treatment group on the probability of making a donation. According to the results, individuals who received the matching grant offer were 0.42 percentage points more likely to donate compared to those in the control group. This effect is statistically significant (p = 0.0019), with a 95% confidence interval ranging from 0.15 to 0.68 percentage points.\nAlthough the effect may appear small in absolute terms, it is meaningful given the low baseline response rate. This suggests that simple behavioral nudges—such as informing donors that their contribution will be matched—can effectively increase participation in charitable giving.\nThis result reinforces the visual difference we observed earlier and aligns with the findings reported in Table 2a Panel A of Karlan and List (2007), confirming the positive impact of matching grants on donor behavior.\n\nTo complement the linear regression, we estimate a probit model for the binary outcome gave, with treatment as the only explanatory variable.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit()\nprobit_model.summary()\n\nparams = probit_model.params\npvals = probit_model.pvalues\nconf_int = probit_model.conf_int()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\nprobit_result = pd.DataFrame({\n    \"Coefficient\": params.round(4),\n    \"P-value\": pvals.round(4),\n    \"95% CI Lower\": conf_int[0].round(4),\n    \"95% CI Upper\": conf_int[1].round(4)\n})\nprobit_result\n\n\n\n\n\n\n\n\nCoefficient\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n-2.1001\n0.0000\n-2.1458\n-2.0544\n\n\ntreatment\n0.0868\n0.0019\n0.0321\n0.1414\n\n\n\n\n\n\n\nThis model confirms that people who were told their donation would be matched were more likely to give. Even though the number looks small, the effect is statistically real—not due to chance. In simple terms, a matching grant makes people feel like their donation “counts more,” and that encourages them to act.\nNote: While our linear regression result closely matches Table 3 Column 1 in Karlan and List (2007), our probit coefficient does not. This may be due to differences in specification, such as use of marginal effects or inclusion of additional covariates in the published model.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo examine whether higher match ratios increase donation likelihood, we perform a series of t-tests within the treatment group.\nWe tested whether different match ratios (1:1, 2:1, and 3:1) led to different donation rates using a series of t-tests within the treatment group. As shown in the table above, none of the comparisons yielded statistically significant differences, with all p-values well above the 0.05 threshold.\nThis suggests that increasing the match size beyond 1:1 did not further motivate people to donate, even though the higher ratios were framed as more generous. The results support the authors’ conclusion on page 8 of Karlan and List (2007), which states that higher match ratios do not significantly improve donation response.\nIn fact, the p-value comparing 2:1 to 3:1 is 0.96, implying that those two offers performed nearly identically in terms of response rate. These findings reinforce the idea that it is the presence of a match—not its magnitude—that drives behavior.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\ntreat = df[df[\"treatment\"] == 1]\n\nrate_1_1 = treat[treat[\"ratio\"] == 1][\"gave\"]\nrate_2_1 = treat[treat[\"ratio\"] == 2][\"gave\"]\nrate_3_1 = treat[treat[\"ratio\"] == 3][\"gave\"]\n\nt_1_2 = ttest_ind(rate_1_1, rate_2_1, equal_var=False)\nt_1_3 = ttest_ind(rate_1_1, rate_3_1, equal_var=False)\nt_2_3 = ttest_ind(rate_2_1, rate_3_1, equal_var=False)\n\n\n\n\n\npd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"],\n    \"T-stat\": [t_1_2.statistic, t_1_3.statistic, t_2_3.statistic],\n    \"P-value\": [t_1_2.pvalue, t_1_3.pvalue, t_2_3.pvalue]\n}).round(4)\n\n\n\n\n\n\n\n\nComparison\nT-stat\nP-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\n\n\n\n\n\nTo complement the t-test analysis, we regress the donation outcome gave on the categorical variable ratio among treatment group members. The regression uses 1:1 as the reference group, and compares 2:1 and 3:1 match offers against it.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport statsmodels.formula.api as smf\ntreat[\"ratio1\"] = (treat[\"ratio\"] == 1).astype(int)\ntreat[\"ratio2\"] = (treat[\"ratio\"] == 2).astype(int)\ntreat[\"ratio3\"] = (treat[\"ratio\"] == 3).astype(int)\n\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat).fit()\n\n\n\n\n\nmodel.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.0207\n0.0014\n14.9122\n0.0000\n0.0180\n0.0235\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n-0.0020\n0.0057\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n-0.0019\n0.0058\n\n\n\n\n\n\n\nAs shown in the table above, neither ratio2 nor ratio3 produces a statistically significant effect on donation response. Both coefficients are small (0.0019 and 0.0020) and have p-values greater than 0.3, indicating no meaningful difference relative to the 1:1 match.\nThese findings reinforce our earlier t-test results and strongly support the authors’ claim in Karlan and List (2007): while the presence of a match increases donations, the size of the match (1:1 vs 2:1 vs 3:1) does not matter.\nFrom the regression coefficients, we estimate the average donation response rate for each match ratio as follows:\n\n1:1 match: 2.07% (intercept)\n2:1 match: 2.26% (0.0207 + 0.0019)\n3:1 match: 2.27% (0.0207 + 0.0020)\n\nThe estimated difference between the 2:1 and 1:1 match ratios is just 0.19 percentage points, and the difference between 3:1 and 2:1 is only 0.01 percentage points. Neither difference is statistically significant (p &gt; 0.3 in both cases).\nThese small, non-significant differences reinforce the finding that increasing the match ratio does not meaningfully affect donor behavior. The results suggest that donors are responsive to the idea of matching, but not particularly sensitive to how large the match offer is.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution\nWe restrict the analysis to individuals who made a donation and regress the donation amount on the treatment indicator. This allows us to assess whether the matching grant affects how much people give, conditional on giving.\nThe regression result shows that donors in the treatment group gave, on average, $1.67 less than those in the control group. However, this difference is not statistically significant (p = 0.56).\nImportantly, the treatment coefficient does not have a clear causal interpretation in this context. This is because we are conditioning on post-treatment behavior (gave == 1), which is itself affected by the treatment. As a result, this analysis describes patterns among donors, but it does not estimate the total causal effect of treatment on donation amount.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndonated = df[df[\"gave\"] == 1]\n\ntreat_amt = donated[donated[\"treatment\"] == 1][\"amount\"]\ncontrol_amt = donated[donated[\"treatment\"] == 0][\"amount\"]\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\nmodel = smf.ols(\"amount ~ treatment\", data=donated).fit()\nmodel.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n45.5403\n2.4234\n18.7921\n0.0000\n40.7850\n50.2956\n\n\ntreatment\n-1.6684\n2.8724\n-0.5808\n0.5615\n-7.3048\n3.9680\n\n\n\n\n\n\n\n\nThe following histograms display the distribution of donation amounts among individuals who donated, separately for the treatment and control groups. In both cases, donations are right-skewed, with most contributions under $100.\nThe red vertical line represents the average donation in each group. As shown, the treatment group donated slightly less on average ($43.87) than the control group ($45.54), but the difference is small and visually negligible.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndonated = df[df[\"gave\"] == 1]\n\ntreat_amt = donated[donated[\"treatment\"] == 1][\"amount\"]\ncontrol_amt = donated[donated[\"treatment\"] == 0][\"amount\"]\n\navg_treat = treat_amt.mean()\navg_control = control_amt.mean()\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nsns.histplot(control_amt, bins=30, color='skyblue')\nplt.axvline(avg_control, color='red', linestyle='--')\nplt.title(\"Control Group: Donation Amount\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(avg_control + 2, plt.ylim()[1]*0.9,\n         f\"Mean: ${avg_control:.2f}\", color=\"red\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nsns.histplot(treat_amt, bins=30, color='lightgreen')\nplt.axvline(avg_treat, color='red', linestyle='--')\nplt.title(\"Treatment Group: Donation Amount\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(avg_treat + 2, plt.ylim()[1]*0.9,\n         f\"Mean: ${avg_treat:.2f}\", color=\"red\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese plots visually reinforce the regression result: matching grants encourage more people to donate, but do not affect how much they give once they’ve decided to donate."
  },
  {
    "objectID": "projects/project2/index.html#simulation-experiment",
    "href": "projects/project2/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe plot following demonstrates the Law of Large Numbers using a simulated donation experiment. We assume that donation rates are 1.8% in the control group and 2.2% in the treatment group. By simulating 10,000 draws from each group, we compute and plot the cumulative average of the differences in donation outcomes.\nAs shown, the cumulative difference is highly volatile at the beginning, fluctuating wildly due to small sample sizes. However, as more observations are included, the average stabilizes and converges toward the true treatment effect of 0.004, marked by the red dashed line.\nThis illustrates a fundamental principle in statistics: with larger sample sizes, the sample average tends to the true population mean. It also explains why we can rely on the t-test in large experiments—because the randomness “averages out.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nn = 10000\ncontrol = np.random.binomial(1, 0.018, n)\ntreatment = np.random.binomial(1, 0.022, n)\n\ndiff = treatment - control\n\ncumulative_avg = np.cumsum(diff) / np.arange(1, n+1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label=\"Cumulative Average (treatment - control)\")\nplt.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.title(\"Simulation: Law of Large Numbers in Action\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Average Difference in Donation Rate\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese four histograms illustrate the Central Limit Theorem using simulated experiments at different sample sizes. For each sample size (50, 200, 500, and 1000), we simulate 1000 experiments by drawing from two Bernoulli distributions (with probabilities 0.018 and 0.022), compute the difference in means, and plot the distribution of these differences.\nAs seen in the plots:\n\nWith small samples (n = 50), the distribution is noisy and non-normal.\nAs the sample size increases, the distribution becomes smoother, more symmetric, and centered around the true treatment effect (0.004), marked by the red dashed line.\nAt n = 1000, the distribution closely resembles a normal curve, confirming the Central Limit Theorem in action.\n\nThis shows that even when the underlying data is binary, the average difference across repeated samples approaches a normal distribution as sample size grows. This is why t-tests are valid in large-sample experiments: the assumptions of approximate normality are satisfied.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(42)\n\ndef simulate_differences(n, reps=1000):\n    diffs = []\n    for _ in range(reps):\n        t = np.random.binomial(1, 0.022, n).mean()\n        c = np.random.binomial(1, 0.018, n).mean()\n        diffs.append(t - c)\n    return np.array(diffs)\n\nsample_sizes = [50, 200, 500, 1000]\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\nfor i, n in enumerate(sample_sizes):\n    diffs = simulate_differences(n)\n    ax = axes[i//2, i%2]\n    sns.histplot(diffs, bins=30, kde=True, ax=ax, color=\"skyblue\")\n    ax.set_title(f\"Sample size = {n}\")\n    ax.axvline(0.004, color='red', linestyle='--', label=\"True Effect\")\n    ax.legend()\n    ax.set_xlabel(\"Estimated Treatment Effect\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation: Distribution of Estimated Treatment Effects\", fontsize=14)\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#conclusion",
    "href": "projects/project2/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis A/B experiment evaluates whether offering a charitable donation match influences giving behavior. Our analysis reveals several key findings:\n\nOffering a matching grant increases the likelihood of donating. Individuals who received the matching offer were significantly more likely to contribute than those in the control group, with a response rate difference of approximately 0.42 percentage points.\nHowever, the size of the match (1:1 vs 2:1 vs 3:1) has no meaningful effect. Neither donation rates nor average donation amounts significantly varied across different match ratios. This suggests that the presence of a match matters more than its magnitude.\nAmong those who did donate, the treatment had no effect on the amount donated. Matching grants seem to increase participation but not donation size.\n\nTogether, these results align with the findings of Karlan and List (2007), reinforcing the idea that psychological cues—such as telling donors their gift will be matched—can nudge people into giving, but once they’re engaged, their level of generosity is guided by other personal factors.\nThis study highlights how behavioral interventions can have measurable effects even when monetary incentives are small, and how careful A/B testing can quantify such effects in real-world settings."
  }
]