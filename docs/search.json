[
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of -0.85.\n\n\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nYe Zheng\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Cars\n\n\n\n\n\n\nYe Zheng\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYe Zheng\n\n\nMay 7, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "YE ZHENG",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign by a liberal nonprofit organization in the United States. A total of 50,083 prior donors were randomly assigned to one of two main groups: a control group, which received a standard solicitation letter, and a treatment group, which received a matching grant offer.\nWithin the treatment group, participants were further randomized into subgroups with different matching ratios ($1:$1, $2:$1, or $3:$1), maximum matching amounts ($25,000, $50,000, $100,000, or unstated), and suggested donation levels (based on their previous giving history). Each donor received a four-page letter with content identical across groups, except for the inclusion or absence of matching grant language.\nThe goal of the experiment was to examine how changes in the price of giving (induced by the matching offers) affected the likelihood and amount of donations. The large sample size and real-world context made this a high-powered natural field experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#introduction",
    "href": "projects/project2/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was conducted through a direct mail campaign by a liberal nonprofit organization in the United States. A total of 50,083 prior donors were randomly assigned to one of two main groups: a control group, which received a standard solicitation letter, and a treatment group, which received a matching grant offer.\nWithin the treatment group, participants were further randomized into subgroups with different matching ratios ($1:$1, $2:$1, or $3:$1), maximum matching amounts ($25,000, $50,000, $100,000, or unstated), and suggested donation levels (based on their previous giving history). Each donor received a four-page letter with content identical across groups, except for the inclusion or absence of matching grant language.\nThe goal of the experiment was to examine how changes in the price of giving (induced by the matching offers) affected the likelihood and amount of donations. The large sample size and real-world context made this a high-powered natural field experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project2/index.html#data",
    "href": "projects/project2/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_stata(\"/Users/kris/Desktop/2025WORK/quarto_website/files/karlan_list_2007.dta\")\n\n\n\n\n\ndf.shape\n\n(50083, 51)\n\n\nThe dataset contains 50,083 observations and 51 variables, each representing a past donor who received a fundraising letter as part of a randomized field experiment. The data were collected to study how matching grants influence charitable giving behavior.\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\ndf['treatment'].value_counts() \n\ntreatment\n1    33396\n0    16687\nName: count, dtype: int64\n\n\nSummary statistics suggest most variables are binary or categorical indicators. Several variables such as cases, nonlit, and couple contain some missing values, which should be considered during modeling. The average suggested donation ranges widely, with variables like ask1 and askd1 showing values between 0 and 1 (indicating binary splits or quantiles).\nOverall, the data are rich in both experimental variation and individual-level covariates, suitable for analyzing treatment effects through regression and stratified comparisons.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nTo verify that the randomization was successful, I perform a balance test on several pre-treatment variables. Specifically, we compare the following variables across the treatment and control groups: - mrm2: months since last donation - freq: number of prior donations - hpa: highest previous contribution - female: gender indicator\nI conduct both t-tests and OLS regressions for each variable to assess statistical significance and confirm consistency across methods.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\nbalance_vars = ['mrm2', 'freq', 'hpa', 'female']\nttest_results = []\nreg_results = []\n\nfor var in balance_vars:\n    t0 = df[df['treatment'] == 0][var].dropna()\n    t1 = df[df['treatment'] == 1][var].dropna()\n    t_stat, p_val = ttest_ind(t1, t0, equal_var=False)\n    ttest_results.append((var, t_stat, p_val))\n\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    reg_results.append((var, model.params['treatment'], model.pvalues['treatment']))\n\n\n\n\nAccording to the test result, none of the variables show statistically significant differences between treatment and control groups at the 5% level. The lowest p-value appears in the case of female (p ≈ 0.079), which is marginally significant under a 10% threshold, but still within an acceptable range for randomization.\nThese results confirm that the groups are generally well balanced across observed characteristics, supporting the internal validity of the experimental design. Importantly, our results align closely with those reported in Table 1 of Karlan and List (2007), which also finds no significant imbalance on these and other pre-treatment covariates.\n\nttest_df = pd.DataFrame(ttest_results, columns=[\"Variable\", \"T-statistic\", \"P-value\"])\nreg_df = pd.DataFrame(reg_results, columns=[\"Variable\", \"Coeff (Regression)\", \"P-value\"])\nttest_df.merge(reg_df, on=\"Variable\")\n\n\n\n\n\n\n\n\nVariable\nT-statistic\nP-value_x\nCoeff (Regression)\nP-value_y\n\n\n\n\n0\nmrm2\n0.119532\n0.904855\n0.013686\n0.904886\n\n\n1\nfreq\n-0.110845\n0.911740\n-0.011979\n0.911702\n\n\n2\nhpa\n0.970427\n0.331840\n0.637075\n0.345099\n\n\n3\nfemale\n-1.753513\n0.079523\n-0.007547\n0.078691"
  },
  {
    "objectID": "projects/project2/index.html#experimental-results",
    "href": "projects/project2/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nTo examine whether matching grants increase the likelihood of donating, I compare the donation response rate between the treatment and control groups. The plot below shows the proportion of individuals who made a donation in each group.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\n\nresponse_rate = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\nresponse_rate[\"Group\"] = response_rate[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nax = sns.barplot(data=response_rate, x=\"Group\", y=\"gave\", palette=\"Set2\")\n\nfor i, val in enumerate(response_rate[\"gave\"]):\n    ax.text(i, val + 0.0015, f\"{val:.2%}\", ha='center', va='bottom', fontsize=12)\n\nplt.title(\"Donation Response Rate by Group\", fontsize=16)\nplt.xlabel(\"Group\", fontsize=14)\nplt.ylabel(\"Proportion Who Donated\", fontsize=14)\nplt.ylim(0, response_rate[\"gave\"].max() * 1.3)\nsns.despine()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe figure above compares the donation response rate between the treatment and control groups. The treatment group, which received a matching grant offer, had a donation rate of 2.20%, while the control group had a rate of 1.79%.\nAlthough the absolute difference in response rates appears modest, it represents a relative increase of over 22%, suggesting that the presence of a matching grant can meaningfully boost the likelihood of charitable giving. This provides early visual evidence supporting the hypothesis that price incentives (in the form of matching) increase donation behavior.\n\nWe now formally test whether individuals in the treatment group were significantly more likely to donate than those in the control group.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\nt1 = df[df[\"treatment\"] == 1][\"gave\"]\nt0 = df[df[\"treatment\"] == 0][\"gave\"]\nt_stat, p_val = ttest_ind(t1, t0, equal_var=False)\n\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoefs = model.params\npvals = model.pvalues\nconf_int = model.conf_int()\nresult_table = pd.DataFrame({\n    \"Coefficient\": coefs.round(4),\n    \"P-value\": pvals.round(4),\n    \"95% CI Lower\": conf_int[0].round(4),\n    \"95% CI Upper\": conf_int[1].round(4)\n})\n\n\n\n\n\nresult_table\n\n\n\n\n\n\n\n\nCoefficient\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.0179\n0.0000\n0.0157\n0.0200\n\n\ntreatment\n0.0042\n0.0019\n0.0015\n0.0068\n\n\n\n\n\n\n\nThe regression model estimates the effect of being in the treatment group on the probability of making a donation. According to the results, individuals who received the matching grant offer were 0.42 percentage points more likely to donate compared to those in the control group. This effect is statistically significant (p = 0.0019), with a 95% confidence interval ranging from 0.15 to 0.68 percentage points.\nAlthough the effect may appear small in absolute terms, it is meaningful given the low baseline response rate. This suggests that simple behavioral nudges—such as informing donors that their contribution will be matched—can effectively increase participation in charitable giving.\nThis result reinforces the visual difference we observed earlier and aligns with the findings reported in Table 2a Panel A of Karlan and List (2007), confirming the positive impact of matching grants on donor behavior.\n\nTo complement the linear regression, we estimate a probit model for the binary outcome gave, with treatment as the only explanatory variable.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit()\nprobit_model.summary()\n\nparams = probit_model.params\npvals = probit_model.pvalues\nconf_int = probit_model.conf_int()\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\nprobit_result = pd.DataFrame({\n    \"Coefficient\": params.round(4),\n    \"P-value\": pvals.round(4),\n    \"95% CI Lower\": conf_int[0].round(4),\n    \"95% CI Upper\": conf_int[1].round(4)\n})\nprobit_result\n\n\n\n\n\n\n\n\nCoefficient\nP-value\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n-2.1001\n0.0000\n-2.1458\n-2.0544\n\n\ntreatment\n0.0868\n0.0019\n0.0321\n0.1414\n\n\n\n\n\n\n\nThis model confirms that people who were told their donation would be matched were more likely to give. Even though the number looks small, the effect is statistically real—not due to chance. In simple terms, a matching grant makes people feel like their donation “counts more,” and that encourages them to act.\nNote: While our linear regression result closely matches Table 3 Column 1 in Karlan and List (2007), our probit coefficient does not. This may be due to differences in specification, such as use of marginal effects or inclusion of additional covariates in the published model.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nTo examine whether higher match ratios increase donation likelihood, we perform a series of t-tests within the treatment group.\nWe tested whether different match ratios (1:1, 2:1, and 3:1) led to different donation rates using a series of t-tests within the treatment group. As shown in the table above, none of the comparisons yielded statistically significant differences, with all p-values well above the 0.05 threshold.\nThis suggests that increasing the match size beyond 1:1 did not further motivate people to donate, even though the higher ratios were framed as more generous. The results support the authors’ conclusion on page 8 of Karlan and List (2007), which states that higher match ratios do not significantly improve donation response.\nIn fact, the p-value comparing 2:1 to 3:1 is 0.96, implying that those two offers performed nearly identically in terms of response rate. These findings reinforce the idea that it is the presence of a match—not its magnitude—that drives behavior.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nfrom scipy.stats import ttest_ind\n\ntreat = df[df[\"treatment\"] == 1]\n\nrate_1_1 = treat[treat[\"ratio\"] == 1][\"gave\"]\nrate_2_1 = treat[treat[\"ratio\"] == 2][\"gave\"]\nrate_3_1 = treat[treat[\"ratio\"] == 3][\"gave\"]\n\nt_1_2 = ttest_ind(rate_1_1, rate_2_1, equal_var=False)\nt_1_3 = ttest_ind(rate_1_1, rate_3_1, equal_var=False)\nt_2_3 = ttest_ind(rate_2_1, rate_3_1, equal_var=False)\n\n\n\n\n\npd.DataFrame({\n    \"Comparison\": [\"1:1 vs 2:1\", \"1:1 vs 3:1\", \"2:1 vs 3:1\"],\n    \"T-stat\": [t_1_2.statistic, t_1_3.statistic, t_2_3.statistic],\n    \"P-value\": [t_1_2.pvalue, t_1_3.pvalue, t_2_3.pvalue]\n}).round(4)\n\n\n\n\n\n\n\n\nComparison\nT-stat\nP-value\n\n\n\n\n0\n1:1 vs 2:1\n-0.9650\n0.3345\n\n\n1\n1:1 vs 3:1\n-1.0150\n0.3101\n\n\n2\n2:1 vs 3:1\n-0.0501\n0.9600\n\n\n\n\n\n\n\n\nTo complement the t-test analysis, we regress the donation outcome gave on the categorical variable ratio among treatment group members. The regression uses 1:1 as the reference group, and compares 2:1 and 3:1 match offers against it.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport statsmodels.formula.api as smf\ntreat[\"ratio1\"] = (treat[\"ratio\"] == 1).astype(int)\ntreat[\"ratio2\"] = (treat[\"ratio\"] == 2).astype(int)\ntreat[\"ratio3\"] = (treat[\"ratio\"] == 3).astype(int)\n\nmodel = smf.ols(\"gave ~ ratio2 + ratio3\", data=treat).fit()\n\n\n\n\n\nmodel.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.0207\n0.0014\n14.9122\n0.0000\n0.0180\n0.0235\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n-0.0020\n0.0057\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n-0.0019\n0.0058\n\n\n\n\n\n\n\nAs shown in the table above, neither ratio2 nor ratio3 produces a statistically significant effect on donation response. Both coefficients are small (0.0019 and 0.0020) and have p-values greater than 0.3, indicating no meaningful difference relative to the 1:1 match.\nThese findings reinforce our earlier t-test results and strongly support the authors’ claim in Karlan and List (2007): while the presence of a match increases donations, the size of the match (1:1 vs 2:1 vs 3:1) does not matter.\nFrom the regression coefficients, we estimate the average donation response rate for each match ratio as follows:\n\n1:1 match: 2.07% (intercept)\n2:1 match: 2.26% (0.0207 + 0.0019)\n3:1 match: 2.27% (0.0207 + 0.0020)\n\nThe estimated difference between the 2:1 and 1:1 match ratios is just 0.19 percentage points, and the difference between 3:1 and 2:1 is only 0.01 percentage points. Neither difference is statistically significant (p &gt; 0.3 in both cases).\nThese small, non-significant differences reinforce the finding that increasing the match ratio does not meaningfully affect donor behavior. The results suggest that donors are responsive to the idea of matching, but not particularly sensitive to how large the match offer is.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution\nWe restrict the analysis to individuals who made a donation and regress the donation amount on the treatment indicator. This allows us to assess whether the matching grant affects how much people give, conditional on giving.\nThe regression result shows that donors in the treatment group gave, on average, $1.67 less than those in the control group. However, this difference is not statistically significant (p = 0.56).\nImportantly, the treatment coefficient does not have a clear causal interpretation in this context. This is because we are conditioning on post-treatment behavior (gave == 1), which is itself affected by the treatment. As a result, this analysis describes patterns among donors, but it does not estimate the total causal effect of treatment on donation amount.\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndonated = df[df[\"gave\"] == 1]\n\ntreat_amt = donated[donated[\"treatment\"] == 1][\"amount\"]\ncontrol_amt = donated[donated[\"treatment\"] == 0][\"amount\"]\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\nmodel = smf.ols(\"amount ~ treatment\", data=donated).fit()\nmodel.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n45.5403\n2.4234\n18.7921\n0.0000\n40.7850\n50.2956\n\n\ntreatment\n-1.6684\n2.8724\n-0.5808\n0.5615\n-7.3048\n3.9680\n\n\n\n\n\n\n\n\nThe following histograms display the distribution of donation amounts among individuals who donated, separately for the treatment and control groups. In both cases, donations are right-skewed, with most contributions under $100.\nThe red vertical line represents the average donation in each group. As shown, the treatment group donated slightly less on average ($43.87) than the control group ($45.54), but the difference is small and visually negligible.\n\n\n\n\n\n\nCoding\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndonated = df[df[\"gave\"] == 1]\n\ntreat_amt = donated[donated[\"treatment\"] == 1][\"amount\"]\ncontrol_amt = donated[donated[\"treatment\"] == 0][\"amount\"]\n\navg_treat = treat_amt.mean()\navg_control = control_amt.mean()\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nsns.histplot(control_amt, bins=30, color='skyblue')\nplt.axvline(avg_control, color='red', linestyle='--')\nplt.title(\"Control Group: Donation Amount\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(avg_control + 2, plt.ylim()[1]*0.9,\n         f\"Mean: ${avg_control:.2f}\", color=\"red\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\nsns.histplot(treat_amt, bins=30, color='lightgreen')\nplt.axvline(avg_treat, color='red', linestyle='--')\nplt.title(\"Treatment Group: Donation Amount\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.text(avg_treat + 2, plt.ylim()[1]*0.9,\n         f\"Mean: ${avg_treat:.2f}\", color=\"red\", fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese plots visually reinforce the regression result: matching grants encourage more people to donate, but do not affect how much they give once they’ve decided to donate."
  },
  {
    "objectID": "projects/project2/index.html#simulation-experiment",
    "href": "projects/project2/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe plot following demonstrates the Law of Large Numbers using a simulated donation experiment. We assume that donation rates are 1.8% in the control group and 2.2% in the treatment group. By simulating 10,000 draws from each group, we compute and plot the cumulative average of the differences in donation outcomes.\nAs shown, the cumulative difference is highly volatile at the beginning, fluctuating wildly due to small sample sizes. However, as more observations are included, the average stabilizes and converges toward the true treatment effect of 0.004, marked by the red dashed line.\nThis illustrates a fundamental principle in statistics: with larger sample sizes, the sample average tends to the true population mean. It also explains why we can rely on the t-test in large experiments—because the randomness “averages out.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nn = 10000\ncontrol = np.random.binomial(1, 0.018, n)\ntreatment = np.random.binomial(1, 0.022, n)\n\ndiff = treatment - control\n\ncumulative_avg = np.cumsum(diff) / np.arange(1, n+1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label=\"Cumulative Average (treatment - control)\")\nplt.axhline(y=0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.title(\"Simulation: Law of Large Numbers in Action\")\nplt.xlabel(\"Sample Size\")\nplt.ylabel(\"Average Difference in Donation Rate\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese four histograms illustrate the Central Limit Theorem using simulated experiments at different sample sizes. For each sample size (50, 200, 500, and 1000), we simulate 1000 experiments by drawing from two Bernoulli distributions (with probabilities 0.018 and 0.022), compute the difference in means, and plot the distribution of these differences.\nAs seen in the plots:\n\nWith small samples (n = 50), the distribution is noisy and non-normal.\nAs the sample size increases, the distribution becomes smoother, more symmetric, and centered around the true treatment effect (0.004), marked by the red dashed line.\nAt n = 1000, the distribution closely resembles a normal curve, confirming the Central Limit Theorem in action.\n\nThis shows that even when the underlying data is binary, the average difference across repeated samples approaches a normal distribution as sample size grows. This is why t-tests are valid in large-sample experiments: the assumptions of approximate normality are satisfied.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(42)\n\ndef simulate_differences(n, reps=1000):\n    diffs = []\n    for _ in range(reps):\n        t = np.random.binomial(1, 0.022, n).mean()\n        c = np.random.binomial(1, 0.018, n).mean()\n        diffs.append(t - c)\n    return np.array(diffs)\n\nsample_sizes = [50, 200, 500, 1000]\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\nfor i, n in enumerate(sample_sizes):\n    diffs = simulate_differences(n)\n    ax = axes[i//2, i%2]\n    sns.histplot(diffs, bins=30, kde=True, ax=ax, color=\"skyblue\")\n    ax.set_title(f\"Sample size = {n}\")\n    ax.axvline(0.004, color='red', linestyle='--', label=\"True Effect\")\n    ax.legend()\n    ax.set_xlabel(\"Estimated Treatment Effect\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Central Limit Theorem Simulation: Distribution of Estimated Treatment Effects\", fontsize=14)\nplt.tight_layout()\nplt.subplots_adjust(top=0.90)\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html#conclusion",
    "href": "projects/project2/index.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis A/B experiment evaluates whether offering a charitable donation match influences giving behavior. Our analysis reveals several key findings:\n\nOffering a matching grant increases the likelihood of donating. Individuals who received the matching offer were significantly more likely to contribute than those in the control group, with a response rate difference of approximately 0.42 percentage points.\nHowever, the size of the match (1:1 vs 2:1 vs 3:1) has no meaningful effect. Neither donation rates nor average donation amounts significantly varied across different match ratios. This suggests that the presence of a match matters more than its magnitude.\nAmong those who did donate, the treatment had no effect on the amount donated. Matching grants seem to increase participation but not donation size.\n\nTogether, these results align with the findings of Karlan and List (2007), reinforcing the idea that psychological cues—such as telling donors their gift will be matched—can nudge people into giving, but once they’re engaged, their level of generosity is guided by other personal factors.\nThis study highlights how behavioral interventions can have measurable effects even when monetary incentives are small, and how careful A/B testing can quantify such effects in real-world settings."
  },
  {
    "objectID": "projects/project3/index.html",
    "href": "projects/project3/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\nblueprinty = pd.read_csv(\"/Users/kris/Desktop/2025WORK/quarto_website/files/blueprinty.csv\")\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe histogram shows that Blueprinty customers (iscustomer = 1) tend to have more patents compared to non-customers. Their distribution is slightly shifted to the right — meaning a higher patent count is more common among users of the software. This visual difference supports the hypothesis that using Blueprinty’s software may help firms generate more patentable innovations. However, the overlap between the groups suggests we need regression to control for confounding factors like age and region before drawing strong conclusions.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 8))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=30)\nplt.title(\"Histogram of Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\nThe average number of patents is higher for Blueprinty customers (4.13) than for non-customers (3.47), indicating a potential benefit from using the software. While this difference supports the company’s claim, it does not yet account for other factors such as firm age or region.\n\nmeans_df = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmeans_df.columns = [\"Customer\", \"Mean Patents\"]\nmeans_df[\"Customer\"] = means_df[\"Customer\"].map({0: \"No\", 1: \"Yes\"})\nmeans_df\n\n\n\n\n\n\n\n\nCustomer\nMean Patents\n\n\n\n\n0\nNo\n3.473013\n\n\n1\nYes\n4.133056\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAccording to the plot, Blueprinty customers are more concentrated in the Northeast region compared to other areas. In contrast, regions like the Midwest and Northwest have mostly non-customers. This suggests that regional factors may influence software adoption, possibly due to marketing outreach, tech infrastructure, or industry presence.\nBecause of this, region should be included as a control variable in any regression modeling to account for potential selection bias.\n\nregion_ct = blueprinty.groupby(['region', 'iscustomer']).size().unstack(fill_value=0)\nregion_ct.plot(kind='bar', stacked=True, figsize=(8, 6))\nplt.title(\"Customer vs Non-Customer Counts by Region\")\nplt.ylabel(\"Firm Count\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Customer\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that Blueprinty customers tend to be slightly younger than non-customers, though the difference is modest. The interquartile range (IQR) is also slightly lower for customers, suggesting less variability in firm age. These age differences might reflect software adoption preferences based on firm maturity or openness to innovation.\nTo avoid biased inference, firm age should be included as a covariate in the Poisson regression model.\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\nplt.title(\"Firm Age Distribution by Customer Status\")\nplt.xlabel(\"Customer\")\nplt.ylabel(\"Firm Age\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet \\(Y_i\\) be the number of patents for firm \\(i\\), assumed to follow a Poisson distribution with mean \\(\\lambda_i\\):\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for all \\(n\\) firms is:\n\\[\n\\mathcal{L}(\\boldsymbol{\\lambda} \\mid \\mathbf{Y}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log \\mathcal{L} = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\nIn a regression model, we typically express \\(\\lambda_i = \\exp(X_i \\beta)\\) so that \\(\\lambda_i &gt; 0\\) for all \\(i\\).\nTo estimate the Poisson regression model via Maximum Likelihood, we need to define the log-likelihood function for the Poisson distribution.\nThe Poisson likelihood assumes that the number of patents follows a distribution where the probability of observing count \\(Y_i\\) depends on an expected rate \\(\\lambda_i\\), which we model as an exponential function of predictors (i.e., \\(\\lambda_i = \\exp(X_i \\beta)\\)).\nThe log-likelihood function (summed over all observations) is:\n\\[\n\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\nIn code, we define a function that takes in the regression coefficients \\(\\beta\\), computes \\(\\lambda_i = \\exp(X_i \\beta)\\), and returns the negative log-likelihood (since most optimization routines perform minimization).\nWe will use this function in the next step to estimate \\(\\beta\\) via numerical optimization.\n\nimport numpy as np\n\ndef poisson_log_likelihood(beta, X, y):\n    \"\"\"\n    beta: coefficient vector (numpy array)\n    X: design matrix (n x p)\n    y: observed count outcomes (n x 1)\n    \"\"\"\n    linear_predictor = X @ beta\n    lambda_ = np.exp(linear_predictor)\n    \n    log_lik = np.sum(y * linear_predictor - lambda_ - np.log(np.math.factorial(y).astype(float)))\n    return -log_lik\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nY_obs = blueprinty[\"patents\"].values\nlambda_range = np.linspace(0.1, 10, 200)\n\nlog_likelihoods = [\n    np.sum(-l + Y_obs * np.log(l) - gammaln(Y_obs + 1))\n    for l in lambda_range\n]\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_range, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the Poisson log-likelihood for different values of \\(\\lambda\\), assuming a constant rate across all firms. The peak of the curve corresponds to the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\). This is where the data is most likely under the Poisson model.\nThe log-likelihood curve peaks at a value of \\(\\lambda\\) around 3–4, indicating that the most likely value of \\(\\lambda\\) under the Poisson model is approximately the sample mean number of patents. This provides an important visual intuition: MLE chooses the parameter that makes the observed data most probable.\nThis curve also demonstrates the key property of MLE: the log-likelihood function is smooth and concave (single maximum), making it well-suited for numerical optimization.\nTo find the MLE analytically for a simple Poisson model with a constant \\(\\lambda\\), we take the derivative of the log-likelihood:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this equal to 0:\n\\[\n\\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0 \\quad \\Rightarrow \\quad \\sum Y_i = n \\lambda \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = \\bar{Y}\n\\]\nSo the MLE of \\(\\lambda\\) is simply the sample mean of \\(Y\\), which matches our earlier visual result.\n\nfrom scipy.optimize import minimize_scalar\n\ndef neg_log_likelihood_scalar(lmbda):\n    return -np.sum(-lmbda + Y_obs * np.log(lmbda) - gammaln(Y_obs + 1))\nresult = minimize_scalar(neg_log_likelihood_scalar, bounds=(0.1, 10), method=\"bounded\")\nlambda_mle = result.x\nlambda_mle\n\n3.6846662261327716\n\n\nUsing numerical optimization, we estimated the MLE of a constant Poisson rate parameter \\(\\lambda\\). The estimated value of \\(\\hat{\\lambda}_{\\text{MLE}} = 3.68\\) is nearly identical to the sample mean of the patent counts. This result aligns with the analytical solution for the Poisson distribution, where the MLE of \\(\\lambda\\) is the mean of \\(Y\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo estimate a full Poisson regression model, we extend our likelihood function to include multiple covariates. Instead of assuming a constant Poisson rate \\(\\lambda\\), we model it as a function of firm characteristics using a log-link:\n\\[\n\\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nIn the code below, we define a log-likelihood function that: - Takes in a vector of coefficients (beta) and a design matrix (X) of covariates, - Computes the predicted rate \\(\\lambda_i\\) as \\(\\exp(X_i^\\top \\beta)\\), - Evaluates the log-likelihood of the Poisson distribution across all observations, - Returns the negative log-likelihood (since most optimizers minimize by default).\nWe will use this function in the next step to estimate the coefficients \\(\\beta\\) via numerical optimization.\n\nfrom scipy.special import gammaln\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, X, y):\n    \"\"\"\n    beta: coefficient vector (p,)\n    X: design matrix (n, p)\n    y: observed count outcomes (n,)\n    \"\"\"\n    eta = X @ beta\n    lam = np.exp(eta)\n    log_lik = np.sum(y * eta - lam - gammaln(y + 1))\n    return -log_lik \n\nIn the following analysis, we used maximum likelihood estimation to fit a Poisson regression model predicting the number of patents awarded to engineering firms. The model includes firm-level characteristics such as age (and age squared), region, and whether the firm is a customer of Blueprinty’s software. The outcome variable is a non-negative count, making the Poisson model an appropriate choice.\nThe estimation was conducted using a custom log-likelihood function and optimized using the BFGS algorithm. During implementation, numerical stability issues were addressed by clipping extreme values of the linear predictor to avoid overflow in the exponential function.\nThe final results indicate that firm age is positively associated with patent output, but the effect diminishes at higher ages, as reflected by the inclusion of a negative quadratic term. Most notably, the model finds a statistically significant and positive relationship between Blueprinty software usage and patent activity: firms using the software tend to produce more patents, holding other factors constant.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\n\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\ny = blueprinty[\"patents\"].values\nX_np = X.values\nX_np = X.astype(float).values\n\ndef poisson_loglikelihood(beta, X, y):\n    beta = np.asarray(beta).reshape(-1)\n    eta = np.clip(X @ beta, -10, 10)\n    lam = np.exp(eta)\n    if np.any(np.isnan(lam)) or np.any(np.isinf(lam)):\n        print(\"△ lam contains NaN or Inf\")\n        return 1e10\n    log_lik = np.sum(y * eta - lam - gammaln(y + 1))\n    return -log_lik\ninit_beta = np.zeros(X_np.shape[1]) \n\nresult = minimize(\n    poisson_loglikelihood,\n    init_beta,\n    args=(X_np, y),\n    method=\"BFGS\"\n)\n\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate (MLE)\": beta_hat,\n    \"Std. Error\": standard_errors\n})\nmle_table\n\n\n\n\n\n\n\n\nVariable\nEstimate (MLE)\nStd. Error\n\n\n\n\n0\nintercept\n-0.509919\n0.185300\n\n\n1\nage\n0.148700\n0.014011\n\n\n2\nage2\n-0.002972\n0.000260\n\n\n3\niscustomer\n0.207606\n0.032162\n\n\n4\nNortheast\n0.029159\n0.045185\n\n\n5\nNorthwest\n-0.017575\n0.055552\n\n\n6\nSouth\n0.056570\n0.053932\n\n\n7\nSouthwest\n0.050577\n0.049116\n\n\n\n\n\n\n\nThe coefficient estimates and standard errors from statsmodels.GLM() match extremely closely with those obtained via manual maximum likelihood estimation. This confirms that the custom log-likelihood function and optimization process were correctly implemented. The consistency across both methods adds confidence to the validity of the results.\n\nimport statsmodels.api as sm\nX_glm = X.astype(float)\nglm_model = sm.GLM(y, X_glm, family=sm.families.Poisson()).fit()\n\nglm_summary = glm_model.summary2().tables[1]\nglm_summary = glm_summary.rename(columns={\n    \"Coef.\": \"Estimate (GLM)\",\n    \"Std.Err.\": \"Std. Error\"\n})[[\"Estimate (GLM)\", \"Std. Error\"]]\n\nglm_summary\n\n\n\n\n\n\n\n\nEstimate (GLM)\nStd. Error\n\n\n\n\nintercept\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\niscustomer\n0.207591\n0.030895\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\n\n\n\n\n\nThe Poisson regression results confirm that firm-level characteristics are meaningfully associated with patent output. The most salient finding is the positive and statistically significant effect of being a Blueprinty customer: controlling for firm age and region, firms using the software tend to produce more patents on average.\nThe estimated effect of age is positive, with a small negative quadratic term, suggesting that patent production increases with firm maturity but at a diminishing rate. Regional effects are present but relatively small in magnitude.\nThe close agreement between the statsmodels.GLM() results and those from our custom MLE implementation provides strong evidence that the model was correctly specified and estimated.\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where all firms are treated as non-customers, and one where all are treated as customers. Using the fitted Poisson model, we predicted the expected number of patents in each case for every firm.\nThe average difference between the two predicted outcomes is 0.79, meaning that—on average—firms using Blueprinty’s software are expected to obtain approximately 0.79 more patents than similar firms that do not use the software.\nThis provides a clear and interpretable measure of the model’s estimated treatment effect, going beyond the log-linear coefficients.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\nX_0 = X_0.astype(float)\nX_1 = X_1.astype(float)\n\ny_pred_0 = glm_model.predict(X_0)\ny_pred_1 = glm_model.predict(X_1)\n\ntreatment_effect = (y_pred_1 - y_pred_0).mean()\ntreatment_effect\n\n0.792768071045253"
  },
  {
    "objectID": "projects/project3/index.html#blueprinty-case-study",
    "href": "projects/project3/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\nblueprinty = pd.read_csv(\"/Users/kris/Desktop/2025WORK/quarto_website/files/blueprinty.csv\")\nblueprinty.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe histogram shows that Blueprinty customers (iscustomer = 1) tend to have more patents compared to non-customers. Their distribution is slightly shifted to the right — meaning a higher patent count is more common among users of the software. This visual difference supports the hypothesis that using Blueprinty’s software may help firms generate more patentable innovations. However, the overlap between the groups suggests we need regression to control for confounding factors like age and region before drawing strong conclusions.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 8))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=30)\nplt.title(\"Histogram of Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\nThe average number of patents is higher for Blueprinty customers (4.13) than for non-customers (3.47), indicating a potential benefit from using the software. While this difference supports the company’s claim, it does not yet account for other factors such as firm age or region.\n\nmeans_df = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\nmeans_df.columns = [\"Customer\", \"Mean Patents\"]\nmeans_df[\"Customer\"] = means_df[\"Customer\"].map({0: \"No\", 1: \"Yes\"})\nmeans_df\n\n\n\n\n\n\n\n\nCustomer\nMean Patents\n\n\n\n\n0\nNo\n3.473013\n\n\n1\nYes\n4.133056\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nAccording to the plot, Blueprinty customers are more concentrated in the Northeast region compared to other areas. In contrast, regions like the Midwest and Northwest have mostly non-customers. This suggests that regional factors may influence software adoption, possibly due to marketing outreach, tech infrastructure, or industry presence.\nBecause of this, region should be included as a control variable in any regression modeling to account for potential selection bias.\n\nregion_ct = blueprinty.groupby(['region', 'iscustomer']).size().unstack(fill_value=0)\nregion_ct.plot(kind='bar', stacked=True, figsize=(8, 6))\nplt.title(\"Customer vs Non-Customer Counts by Region\")\nplt.ylabel(\"Firm Count\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Customer\", labels=[\"No\", \"Yes\"])\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplot shows that Blueprinty customers tend to be slightly younger than non-customers, though the difference is modest. The interquartile range (IQR) is also slightly lower for customers, suggesting less variability in firm age. These age differences might reflect software adoption preferences based on firm maturity or openness to innovation.\nTo avoid biased inference, firm age should be included as a covariate in the Poisson regression model.\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=blueprinty, x=\"iscustomer\", y=\"age\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\nplt.title(\"Firm Age Distribution by Customer Status\")\nplt.xlabel(\"Customer\")\nplt.ylabel(\"Firm Age\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet \\(Y_i\\) be the number of patents for firm \\(i\\), assumed to follow a Poisson distribution with mean \\(\\lambda_i\\):\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\nThe probability mass function is:\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAssuming independence across observations, the likelihood function for all \\(n\\) firms is:\n\\[\n\\mathcal{L}(\\boldsymbol{\\lambda} \\mid \\mathbf{Y}) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nAnd the log-likelihood is:\n\\[\n\\log \\mathcal{L} = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\nIn a regression model, we typically express \\(\\lambda_i = \\exp(X_i \\beta)\\) so that \\(\\lambda_i &gt; 0\\) for all \\(i\\).\nTo estimate the Poisson regression model via Maximum Likelihood, we need to define the log-likelihood function for the Poisson distribution.\nThe Poisson likelihood assumes that the number of patents follows a distribution where the probability of observing count \\(Y_i\\) depends on an expected rate \\(\\lambda_i\\), which we model as an exponential function of predictors (i.e., \\(\\lambda_i = \\exp(X_i \\beta)\\)).\nThe log-likelihood function (summed over all observations) is:\n\\[\n\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n\\]\nIn code, we define a function that takes in the regression coefficients \\(\\beta\\), computes \\(\\lambda_i = \\exp(X_i \\beta)\\), and returns the negative log-likelihood (since most optimization routines perform minimization).\nWe will use this function in the next step to estimate \\(\\beta\\) via numerical optimization.\n\nimport numpy as np\n\ndef poisson_log_likelihood(beta, X, y):\n    \"\"\"\n    beta: coefficient vector (numpy array)\n    X: design matrix (n x p)\n    y: observed count outcomes (n x 1)\n    \"\"\"\n    linear_predictor = X @ beta\n    lambda_ = np.exp(linear_predictor)\n    \n    log_lik = np.sum(y * linear_predictor - lambda_ - np.log(np.math.factorial(y).astype(float)))\n    return -log_lik\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nY_obs = blueprinty[\"patents\"].values\nlambda_range = np.linspace(0.1, 10, 200)\n\nlog_likelihoods = [\n    np.sum(-l + Y_obs * np.log(l) - gammaln(Y_obs + 1))\n    for l in lambda_range\n]\n\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_range, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows the Poisson log-likelihood for different values of \\(\\lambda\\), assuming a constant rate across all firms. The peak of the curve corresponds to the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\). This is where the data is most likely under the Poisson model.\nThe log-likelihood curve peaks at a value of \\(\\lambda\\) around 3–4, indicating that the most likely value of \\(\\lambda\\) under the Poisson model is approximately the sample mean number of patents. This provides an important visual intuition: MLE chooses the parameter that makes the observed data most probable.\nThis curve also demonstrates the key property of MLE: the log-likelihood function is smooth and concave (single maximum), making it well-suited for numerical optimization.\nTo find the MLE analytically for a simple Poisson model with a constant \\(\\lambda\\), we take the derivative of the log-likelihood:\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n\\]\nSetting this equal to 0:\n\\[\n\\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right) = 0 \\quad \\Rightarrow \\quad \\sum Y_i = n \\lambda \\quad \\Rightarrow \\quad \\lambda_{\\text{MLE}} = \\bar{Y}\n\\]\nSo the MLE of \\(\\lambda\\) is simply the sample mean of \\(Y\\), which matches our earlier visual result.\n\nfrom scipy.optimize import minimize_scalar\n\ndef neg_log_likelihood_scalar(lmbda):\n    return -np.sum(-lmbda + Y_obs * np.log(lmbda) - gammaln(Y_obs + 1))\nresult = minimize_scalar(neg_log_likelihood_scalar, bounds=(0.1, 10), method=\"bounded\")\nlambda_mle = result.x\nlambda_mle\n\n3.6846662261327716\n\n\nUsing numerical optimization, we estimated the MLE of a constant Poisson rate parameter \\(\\lambda\\). The estimated value of \\(\\hat{\\lambda}_{\\text{MLE}} = 3.68\\) is nearly identical to the sample mean of the patent counts. This result aligns with the analytical solution for the Poisson distribution, where the MLE of \\(\\lambda\\) is the mean of \\(Y\\).\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo estimate a full Poisson regression model, we extend our likelihood function to include multiple covariates. Instead of assuming a constant Poisson rate \\(\\lambda\\), we model it as a function of firm characteristics using a log-link:\n\\[\n\\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nIn the code below, we define a log-likelihood function that: - Takes in a vector of coefficients (beta) and a design matrix (X) of covariates, - Computes the predicted rate \\(\\lambda_i\\) as \\(\\exp(X_i^\\top \\beta)\\), - Evaluates the log-likelihood of the Poisson distribution across all observations, - Returns the negative log-likelihood (since most optimizers minimize by default).\nWe will use this function in the next step to estimate the coefficients \\(\\beta\\) via numerical optimization.\n\nfrom scipy.special import gammaln\nimport numpy as np\n\ndef poisson_regression_loglikelihood(beta, X, y):\n    \"\"\"\n    beta: coefficient vector (p,)\n    X: design matrix (n, p)\n    y: observed count outcomes (n,)\n    \"\"\"\n    eta = X @ beta\n    lam = np.exp(eta)\n    log_lik = np.sum(y * eta - lam - gammaln(y + 1))\n    return -log_lik \n\nIn the following analysis, we used maximum likelihood estimation to fit a Poisson regression model predicting the number of patents awarded to engineering firms. The model includes firm-level characteristics such as age (and age squared), region, and whether the firm is a customer of Blueprinty’s software. The outcome variable is a non-negative count, making the Poisson model an appropriate choice.\nThe estimation was conducted using a custom log-likelihood function and optimized using the BFGS algorithm. During implementation, numerical stability issues were addressed by clipping extreme values of the linear predictor to avoid overflow in the exponential function.\nThe final results indicate that firm age is positively associated with patent output, but the effect diminishes at higher ages, as reflected by the inclusion of a negative quadratic term. Most notably, the model finds a statistically significant and positive relationship between Blueprinty software usage and patent activity: firms using the software tend to produce more patents, holding other factors constant.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\n\nblueprinty[\"age2\"] = blueprinty[\"age\"] ** 2\n\nregion_dummies = pd.get_dummies(blueprinty[\"region\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=blueprinty.index, name=\"intercept\"),\n    blueprinty[[\"age\", \"age2\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\n\ny = blueprinty[\"patents\"].values\nX_np = X.values\nX_np = X.astype(float).values\n\ndef poisson_loglikelihood(beta, X, y):\n    beta = np.asarray(beta).reshape(-1)\n    eta = np.clip(X @ beta, -10, 10)\n    lam = np.exp(eta)\n    if np.any(np.isnan(lam)) or np.any(np.isinf(lam)):\n        print(\"△ lam contains NaN or Inf\")\n        return 1e10\n    log_lik = np.sum(y * eta - lam - gammaln(y + 1))\n    return -log_lik\ninit_beta = np.zeros(X_np.shape[1]) \n\nresult = minimize(\n    poisson_loglikelihood,\n    init_beta,\n    args=(X_np, y),\n    method=\"BFGS\"\n)\n\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\nmle_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Estimate (MLE)\": beta_hat,\n    \"Std. Error\": standard_errors\n})\nmle_table\n\n\n\n\n\n\n\n\nVariable\nEstimate (MLE)\nStd. Error\n\n\n\n\n0\nintercept\n-0.509919\n0.185300\n\n\n1\nage\n0.148700\n0.014011\n\n\n2\nage2\n-0.002972\n0.000260\n\n\n3\niscustomer\n0.207606\n0.032162\n\n\n4\nNortheast\n0.029159\n0.045185\n\n\n5\nNorthwest\n-0.017575\n0.055552\n\n\n6\nSouth\n0.056570\n0.053932\n\n\n7\nSouthwest\n0.050577\n0.049116\n\n\n\n\n\n\n\nThe coefficient estimates and standard errors from statsmodels.GLM() match extremely closely with those obtained via manual maximum likelihood estimation. This confirms that the custom log-likelihood function and optimization process were correctly implemented. The consistency across both methods adds confidence to the validity of the results.\n\nimport statsmodels.api as sm\nX_glm = X.astype(float)\nglm_model = sm.GLM(y, X_glm, family=sm.families.Poisson()).fit()\n\nglm_summary = glm_model.summary2().tables[1]\nglm_summary = glm_summary.rename(columns={\n    \"Coef.\": \"Estimate (GLM)\",\n    \"Std.Err.\": \"Std. Error\"\n})[[\"Estimate (GLM)\", \"Std. Error\"]]\n\nglm_summary\n\n\n\n\n\n\n\n\nEstimate (GLM)\nStd. Error\n\n\n\n\nintercept\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\niscustomer\n0.207591\n0.030895\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\n\n\n\n\n\nThe Poisson regression results confirm that firm-level characteristics are meaningfully associated with patent output. The most salient finding is the positive and statistically significant effect of being a Blueprinty customer: controlling for firm age and region, firms using the software tend to produce more patents on average.\nThe estimated effect of age is positive, with a small negative quadratic term, suggesting that patent production increases with firm maturity but at a diminishing rate. Regional effects are present but relatively small in magnitude.\nThe close agreement between the statsmodels.GLM() results and those from our custom MLE implementation provides strong evidence that the model was correctly specified and estimated.\n\nTo better interpret the effect of Blueprinty’s software, we created two counterfactual datasets: one where all firms are treated as non-customers, and one where all are treated as customers. Using the fitted Poisson model, we predicted the expected number of patents in each case for every firm.\nThe average difference between the two predicted outcomes is 0.79, meaning that—on average—firms using Blueprinty’s software are expected to obtain approximately 0.79 more patents than similar firms that do not use the software.\nThis provides a clear and interpretable measure of the model’s estimated treatment effect, going beyond the log-linear coefficients.\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\nX_0 = X_0.astype(float)\nX_1 = X_1.astype(float)\n\ny_pred_0 = glm_model.predict(X_0)\ny_pred_1 = glm_model.predict(X_1)\n\ntreatment_effect = (y_pred_1 - y_pred_0).mean()\ntreatment_effect\n\n0.792768071045253"
  },
  {
    "objectID": "projects/project3/index.html#airbnb-case-study",
    "href": "projects/project3/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\nairbnb = pd.read_csv(\"/Users/kris/Desktop/2025WORK/quarto_website/files/airbnb.csv\")\nairbnb.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\nWe begin by examining the structure of the Airbnb dataset, which contains 40,628 listings scraped in 2017. The outcome variable of interest is number_of_reviews, used as a proxy for booking demand.\nKey observations:\nThe variable number_of_reviews is highly right-skewed, with a mean of 15.9 but a maximum of 421, indicating a few listings receive disproportionately many reviews.\nThe variable price ranges from $10 to $10,000, with a median of $100 and strong right skew (mean: 144.8, std: 210.6), suggesting the need for possible trimming or log transformation.\ndays (days since listing) has a very wide range, from 8 to 42,828, again suggesting potential skew.\nReview scores (cleanliness, location, value) are centered near the high end of the 1–10 scale, with means between 9.2–9.4 and relatively low variance — indicating high ratings overall but limited variation.\nSome numeric variables such as bathrooms, bedrooms, and all three review score variables contain missing values. These will need to be dropped or imputed before modeling.\nCategorical variables such as room_type and instant_bookable are stored as object types and will be converted to numeric formats using dummy coding or binary encoding.\nThis analysis helps identify necessary data cleaning steps and informs the variable selection for the Poisson regression model that follows.\n\nairbnb.info()\n\nairbnb[[\n    \"number_of_reviews\", \"days\", \"price\", \"bathrooms\",\n    \"bedrooms\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\"\n]].describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\ndays\nprice\nbathrooms\nbedrooms\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.000000\n40628.000000\n40628.000000\n40468.000000\n40552.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n15.904426\n1102.368219\n144.760732\n1.124592\n1.147046\n9.198370\n9.413544\n9.331522\n\n\nstd\n29.246009\n1383.269358\n210.657597\n0.385884\n0.691746\n1.119935\n0.844949\n0.902966\n\n\nmin\n0.000000\n1.000000\n10.000000\n0.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n1.000000\n542.000000\n70.000000\n1.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n4.000000\n996.000000\n100.000000\n1.000000\n1.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n17.000000\n1535.000000\n170.000000\n1.000000\n1.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n421.000000\n42828.000000\n10000.000000\n8.000000\n10.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\nSeveral variables in the Airbnb dataset contain missing values:\n\nreview_scores_cleanliness, review_scores_location, and review_scores_value each have over 10,000 missing entries (about 25% of the data).\nbathrooms and bedrooms also contain some missing values (160 and 76 respectively).\nhost_since is missing in 35 rows, though we are not using this field directly.\n\nThese missing values are particularly important for modeling, as they affect key predictors. For simplicity and consistency, we choose to drop rows with any missing values in the variables used for modeling.\nAlternatively, imputation strategies such as using median values for bathrooms or mean scores for review_scores_* could be considered, but we proceed with row-wise deletion to maintain model interpretability and avoid introducing bias from artificial imputations.\n\nairbnb.isna().sum()\n\nUnnamed: 0                       0\nid                               0\ndays                             0\nlast_scraped                     0\nhost_since                      35\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\n\nThe histogram shows that the vast majority of Airbnb listings in New York City receive fewer than 20 reviews. The distribution is highly right-skewed, with most listings receiving only a handful of reviews and a long tail of higher values.\nBy capping the x-axis at 100, we can better observe the density of low-review listings, which would otherwise be compressed by a few high-outlier values. This supports the use of a Poisson regression model, which is appropriate for modeling overdispersed count data with many small values and a long right tail.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 分布：number_of_reviews\nplt.figure(figsize=(8, 6))\nsns.histplot(airbnb[\"number_of_reviews\"], bins=20)\nplt.xlim(0, 100)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram of listing prices reveals a highly right-skewed distribution. The vast majority of listings are priced below $200 per night, with a dense concentration under $100. A smaller portion of listings are priced between $200 and $500, and only a few outliers extend beyond that range (not shown here for clarity).\nThis pattern suggests that while most Airbnb properties are relatively affordable, there is a long tail of high-priced listings that could distort analysis if not handled carefully. As a result, we may consider winsorizing, transforming, or clipping the price variable, or using its logarithm in later modeling to reduce the influence of extreme values.\n\nplt.figure(figsize=(8, 6))\nsns.histplot(airbnb[\"price\"], bins=50)\nplt.xlim(0, 500)\nplt.title(\"Distribution of Price\")\nplt.xlabel(\"Price ($)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nThe Poisson regression model estimates how various listing features are associated with the expected number of reviews. These include listing duration, price, review scores, room type, and instant bookability. Coefficients represent changes in the log expected review count. In the next step, we will convert these to IRRs for easier interpretation.\n\nvars_for_model = [\n    \"number_of_reviews\", \"days\", \"price\", \"room_type\", \"instant_bookable\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"bathrooms\", \"bedrooms\"\n]\nairbnb_model = airbnb[vars_for_model].dropna().copy()\n\nairbnb_model[\"instant_bookable\"] = (airbnb_model[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(airbnb_model[\"room_type\"], drop_first=True)\n\nX = pd.concat([\n    pd.Series(1, index=airbnb_model.index, name=\"intercept\"),\n    airbnb_model[[\n        \"days\", \"price\", \"review_scores_cleanliness\",\n        \"review_scores_location\", \"review_scores_value\",\n        \"bathrooms\", \"bedrooms\", \"instant_bookable\"\n    ]],\n    room_dummies\n], axis=1).astype(float)\n\ny = airbnb_model[\"number_of_reviews\"]\n\nThe Poisson regression model estimates how various listing attributes relate to the expected number of reviews. Since the model is log-linear, coefficients represent the log change in expected review count for a one-unit increase in the predictor.\nKey takeaways:\n\ninstant_bookable has a positive and significant coefficient (0.346), suggesting listings that can be booked instantly receive more reviews on average.\nreview_scores_cleanliness is also positively associated with review count, while location and value scores show small negative coefficients — potentially due to collinearity or high baseline ratings.\nprice has a very small negative effect, indicating higher prices may slightly reduce review frequency.\nroom type matters: compared to Entire home/apt (the baseline), Shared room listings have substantially fewer reviews, and Private room has a small negative effect.\n\nOverall, the model indicates that features tied to convenience and perceived cleanliness play an important role in driving demand.\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\nglm_summary = glm_model.summary2().tables[1]\nglm_summary = glm_summary.rename(columns={\n    \"Coef.\": \"Estimate (GLM)\", \"Std.Err.\": \"Std. Error\"\n})[[\"Estimate (GLM)\", \"Std. Error\"]]\nglm_summary\n\n\n\n\n\n\n\n\nEstimate (GLM)\nStd. Error\n\n\n\n\nintercept\n3.498049\n1.609066e-02\n\n\ndays\n0.000051\n3.909218e-07\n\n\nprice\n-0.000018\n8.326458e-06\n\n\nreview_scores_cleanliness\n0.113139\n1.496336e-03\n\n\nreview_scores_location\n-0.076899\n1.608903e-03\n\n\nreview_scores_value\n-0.091076\n1.803855e-03\n\n\nbathrooms\n-0.117704\n3.749225e-03\n\n\nbedrooms\n0.074087\n1.991742e-03\n\n\ninstant_bookable\n0.345850\n2.890138e-03\n\n\nPrivate room\n-0.010536\n2.738448e-03\n\n\nShared room\n-0.246337\n8.619793e-03"
  },
  {
    "objectID": "projects/project3/index.html#summary",
    "href": "projects/project3/index.html#summary",
    "title": "Poisson Regression Examples",
    "section": "Summary",
    "text": "Summary\n🔷 Blueprinty Case\nWe modeled the number of patents awarded to engineering firms as a function of firm characteristics. Using both custom maximum likelihood estimation and validation via statsmodels.GLM(), we found that:\nBlueprinty customers, on average, receive significantly more patents than non-customers, even after controlling for firm age and region.\nThe effect of firm age on patent activity is positive but diminishing, as shown by the inclusion of a negative quadratic age term.\nA counterfactual simulation revealed that if all firms used Blueprinty’s software, they would average 0.79 more patents than if none did.\nThese results support Blueprinty’s marketing claim and demonstrate how predictive modeling can be used to quantify software ROI.\n🔷 Airbnb Case\nWe used the number of reviews as a proxy for booking volume and modeled review counts using Poisson regression. After cleaning the data and engineering features, we found that:\nInstant bookable listings are significantly more likely to receive reviews, suggesting that booking friction reduces demand.\nCleanliness ratings are positively associated with reviews, while other review scores (like location and value) have smaller or negative effects.\nRoom type matters: shared rooms underperform relative to entire homes or private rooms.\nPrice has a small but negative effect, consistent with typical demand elasticity.\nThese insights help explain what makes an Airbnb listing more successful, and demonstrate the utility of Poisson regression in digital marketplace analytics."
  }
]